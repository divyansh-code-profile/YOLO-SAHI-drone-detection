{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2cD-I_JFX7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807c1c47-7272-4744-91e7-e359baec0db3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall pillow\n",
        "!pip install \"pillow<7\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zg1js_fp9fi",
        "outputId": "8e518c10-edcf-45b2-db6c-319b8bac0a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: Pillow 9.4.0\n",
            "Uninstalling Pillow-9.4.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.8/dist-packages/PIL/*\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow-9.4.0.dist-info/*\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/libXau-00ec42fe.so.6.0.0\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/libbrotlicommon-cf2297e4.so.1.0.9\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/libbrotlidec-97e69943.so.1.0.9\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/libfreetype-2a21fedc.so.6.18.3\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/libharfbuzz-c78ddc5e.so.0.60000.0\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/libjpeg-2c0fa17f.so.62.3.0\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/liblcms2-f8fefe53.so.2.0.14\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/liblzma-85b6360a.so.5.4.0\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/libopenjp2-fca9bf24.so.2.5.0\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/libpng16-021811b1.so.16.39.0\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/libtiff-ac0c3d92.so.6.0.0\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/libwebp-b246aa5a.so.7.1.5\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/libwebpdemux-ac83b303.so.2.0.11\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/libwebpmux-502c7428.so.3.0.10\n",
            "    /usr/local/lib/python3.8/dist-packages/Pillow.libs/libxcb-421a6fdb.so.1.1.0\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled Pillow-9.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pillow<7\n",
            "  Using cached Pillow-6.2.2-cp38-cp38-manylinux1_x86_64.whl (2.1 MB)\n",
            "Installing collected packages: pillow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sahi 0.11.9 requires pillow>=8.2.0, but you have pillow 6.2.2 which is incompatible.\n",
            "bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 6.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pillow-6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sahi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "fIIqU-Py2pB-",
        "outputId": "7a9e76df-e60b-4c12-dd2e-53e817631135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sahi in /usr/local/lib/python3.8/dist-packages (0.11.9)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.8/dist-packages (from sahi) (0.5.0)\n",
            "Requirement already satisfied: click==8.0.4 in /usr/local/lib/python3.8/dist-packages (from sahi) (8.0.4)\n",
            "Requirement already satisfied: opencv-python>=4.2.0.32 in /usr/local/lib/python3.8/dist-packages (from sahi) (4.6.0.66)\n",
            "Requirement already satisfied: pybboxes==0.1.5 in /usr/local/lib/python3.8/dist-packages (from sahi) (0.1.5)\n",
            "Collecting pillow>=8.2.0\n",
            "  Using cached Pillow-9.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from sahi) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from sahi) (2.25.1)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.8/dist-packages (from sahi) (3.1.10)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.8/dist-packages (from sahi) (4.64.1)\n",
            "Requirement already satisfied: shapely>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from sahi) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pybboxes==0.1.5->sahi) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from fire->sahi) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire->sahi) (2.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->sahi) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->sahi) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->sahi) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->sahi) (4.0.0)\n",
            "Installing collected packages: pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 6.2.2\n",
            "    Uninstalling Pillow-6.2.2:\n",
            "      Successfully uninstalled Pillow-6.2.2\n",
            "Successfully installed pillow-9.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/yolov7')\n",
        "\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "\n",
        "\n",
        "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = img.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "    return img, ratio, (dw, dh)"
      ],
      "metadata": {
        "id": "GPnup40xFeYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes_to_filter = None  #You can give list of classes to filter by name, Be happy you don't have to put class number. ['train','person' ]\n",
        "\n",
        "\n",
        "opt  = {\n",
        "\n",
        "    \"weights\": \"/content/gdrive/MyDrive/yolov7/runs/train/exp32/weights/best.pt\", # Path to weights file default weights are for nano model\n",
        "    \"yaml\"   : \"/content/gdrive/MyDrive/yolov7/data.yaml\",\n",
        "    \"img-size\": 928, # default image size\n",
        "    \"conf-thres\": 0.25, # confidence threshold for inference.\n",
        "    \"iou-thres\" : 0.45, # NMS IoU threshold for inference.\n",
        "    \"device\" : '0',  # device to run our model i.e. 0 or 0,1,2,3 or cpu\n",
        "    \"classes\" : classes_to_filter  # list of classes to filter or None\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "CFNZkx_OFpBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfejrpNV2oD1",
        "outputId": "b61a6efd-6542-41fd-f72f-53ca920289a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: numpy<1.24.0,>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 5)) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 6)) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 7)) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 8)) (6.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 9)) (2.25.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 10)) (1.7.3)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 11)) (1.13.0+cu116)\n",
            "Requirement already satisfied: torchvision!=0.13.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 12)) (0.14.0+cu116)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 13)) (4.64.1)\n",
            "Requirement already satisfied: protobuf<4.21.3 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 14)) (3.19.6)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (2.9.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 21)) (1.3.5)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 22)) (0.11.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 34)) (7.9.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 35)) (5.4.8)\n",
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 9)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 9)) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 9)) (4.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.7.0->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 11)) (4.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (1.51.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (2.15.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.4->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 21)) (2022.7)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 34)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 34)) (2.0.10)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 34)) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 34)) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 34)) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 34)) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 34)) (0.7.5)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 34)) (0.8.3)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (5.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 34)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 34)) (0.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt (line 17)) (3.2.2)\n",
            "Installing collected packages: jedi, thop\n",
            "Successfully installed jedi-0.18.2 thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sahi.predict import get_sliced_prediction, predict, get_prediction\n",
        "from sahi.utils.file import download_from_url\n",
        "from sahi.utils.cv import read_image\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "oMgBx3w82v2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OBSS SAHI Tool\n",
        "# Code written by Fatih C Akyon, 2020.\n",
        "\n",
        "import logging\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import pybboxes.functional as pbf\n",
        "\n",
        "from sahi.prediction import ObjectPrediction\n",
        "from sahi.utils.compatibility import fix_full_shape_list, fix_shift_amount_list\n",
        "from sahi.utils.cv import get_bbox_from_bool_mask\n",
        "from sahi.utils.import_utils import check_requirements, is_available\n",
        "from sahi.utils.torch import is_torch_cuda_available\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class DetectionModel:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: Optional[str] = None,\n",
        "        model: Optional[Any] = None,\n",
        "        config_path: Optional[str] = None,\n",
        "        device: Optional[str] = None,\n",
        "        mask_threshold: float = 0.5,\n",
        "        confidence_threshold: float = 0.3,\n",
        "        category_mapping: Optional[Dict] = None,\n",
        "        category_remapping: Optional[Dict] = None,\n",
        "        load_at_init: bool = True,\n",
        "        image_size: int = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Init object detection/instance segmentation model.\n",
        "        Args:\n",
        "            model_path: str\n",
        "                Path for the instance segmentation model weight\n",
        "            config_path: str\n",
        "                Path for the mmdetection instance segmentation model config file\n",
        "            device: str\n",
        "                Torch device, \"cpu\" or \"cuda\"\n",
        "            mask_threshold: float\n",
        "                Value to threshold mask pixels, should be between 0 and 1\n",
        "            confidence_threshold: float\n",
        "                All predictions with score < confidence_threshold will be discarded\n",
        "            category_mapping: dict: str to str\n",
        "                Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n",
        "            category_remapping: dict: str to int\n",
        "                Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n",
        "            load_at_init: bool\n",
        "                If True, automatically loads the model at initalization\n",
        "            image_size: int\n",
        "                Inference input size.\n",
        "        \"\"\"\n",
        "        self.model_path = model_path\n",
        "        self.config_path = config_path\n",
        "        self.model = None\n",
        "        self.device = device\n",
        "        self.mask_threshold = mask_threshold\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.category_mapping = category_mapping\n",
        "        self.category_remapping = category_remapping\n",
        "        self.image_size = image_size\n",
        "        self._original_predictions = None\n",
        "        self._object_prediction_list_per_image = None\n",
        "\n",
        "        # automatically set device if its None\n",
        "        if not (self.device):\n",
        "            self.device = \"cuda:0\" if is_torch_cuda_available() else \"cpu\"\n",
        "\n",
        "        # automatically load model if load_at_init is True\n",
        "        if load_at_init:\n",
        "            if model:\n",
        "                self.set_model(model)\n",
        "            else:\n",
        "                self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"\n",
        "        This function should be implemented in a way that detection model\n",
        "        should be initialized and set to self.model.\n",
        "        (self.model_path, self.config_path, and self.device should be utilized)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def set_model(self, model: Any, **kwargs):\n",
        "        \"\"\"\n",
        "        This function should be implemented to instantiate a DetectionModel out of an already loaded model\n",
        "        Args:\n",
        "            model: Any\n",
        "                Loaded model\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def unload_model(self):\n",
        "        \"\"\"\n",
        "        Unloads the model from CPU/GPU.\n",
        "        \"\"\"\n",
        "        self.model = None\n",
        "        if is_available(\"torch\"):\n",
        "            from sahi.utils.torch import empty_cuda_cache\n",
        "\n",
        "            empty_cuda_cache()\n",
        "\n",
        "    def perform_inference(self, image: np.ndarray):\n",
        "        \"\"\"\n",
        "        This function should be implemented in a way that prediction should be\n",
        "        performed using self.model and the prediction result should be set to self._original_predictions.\n",
        "        Args:\n",
        "            image: np.ndarray\n",
        "                A numpy array that contains the image to be predicted.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _create_object_prediction_list_from_original_predictions(\n",
        "        self,\n",
        "        shift_amount_list: Optional[List[List[int]]] = [[0, 0]],\n",
        "        full_shape_list: Optional[List[List[int]]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This function should be implemented in a way that self._original_predictions should\n",
        "        be converted to a list of prediction.ObjectPrediction and set to\n",
        "        self._object_prediction_list. self.mask_threshold can also be utilized.\n",
        "        Args:\n",
        "            shift_amount_list: list of list\n",
        "                To shift the box and mask predictions from sliced image to full sized image, should\n",
        "                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n",
        "            full_shape_list: list of list\n",
        "                Size of the full image after shifting, should be in the form of\n",
        "                List[[height, width],[height, width],...]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _apply_category_remapping(self):\n",
        "        \"\"\"\n",
        "        Applies category remapping based on mapping given in self.category_remapping\n",
        "        \"\"\"\n",
        "        # confirm self.category_remapping is not None\n",
        "        if self.category_remapping is None:\n",
        "            raise ValueError(\"self.category_remapping cannot be None\")\n",
        "        # remap categories\n",
        "        for object_prediction_list in self._object_prediction_list_per_image:\n",
        "            for object_prediction in object_prediction_list:\n",
        "                old_category_id_str = str(object_prediction.category.id)\n",
        "                new_category_id_int = self.category_remapping[old_category_id_str]\n",
        "                object_prediction.category.id = new_category_id_int\n",
        "\n",
        "    def convert_original_predictions(\n",
        "        self,\n",
        "        shift_amount: Optional[List[int]] = [0, 0],\n",
        "        full_shape: Optional[List[int]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Converts original predictions of the detection model to a list of\n",
        "        prediction.ObjectPrediction object. Should be called after perform_inference().\n",
        "        Args:\n",
        "            shift_amount: list\n",
        "                To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]\n",
        "            full_shape: list\n",
        "                Size of the full image after shifting, should be in the form of [height, width]\n",
        "        \"\"\"\n",
        "        self._create_object_prediction_list_from_original_predictions(\n",
        "            shift_amount_list=shift_amount,\n",
        "            full_shape_list=full_shape,\n",
        "        )\n",
        "        if self.category_remapping:\n",
        "            self._apply_category_remapping()\n",
        "\n",
        "    @property\n",
        "    def object_prediction_list(self):\n",
        "        return self._object_prediction_list_per_image[0]\n",
        "\n",
        "    @property\n",
        "    def object_prediction_list_per_image(self):\n",
        "        return self._object_prediction_list_per_image\n",
        "\n",
        "    @property\n",
        "    def original_predictions(self):\n",
        "        return self._original_predictions\n",
        "\n",
        "\n",
        "class MmdetDetectionModel(DetectionModel):\n",
        "    def load_model(self):\n",
        "        \"\"\"\n",
        "        Detection model is initialized and set to self.model.\n",
        "        \"\"\"\n",
        "        check_requirements([\"torch\", \"mmdet\", \"mmcv\"])\n",
        "\n",
        "        from mmdet.apis import init_detector\n",
        "\n",
        "        # create model\n",
        "        model = init_detector(\n",
        "            config=self.config_path,\n",
        "            checkpoint=self.model_path,\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        # update model image size\n",
        "        if self.image_size is not None:\n",
        "            model.cfg.data.test.pipeline[1][\"img_scale\"] = (self.image_size, self.image_size)\n",
        "\n",
        "        self.set_model(model)\n",
        "\n",
        "    def set_model(self, model: Any):\n",
        "        \"\"\"\n",
        "        Sets the underlying MMDetection model.\n",
        "        Args:\n",
        "            model: Any\n",
        "                A MMDetection model\n",
        "        \"\"\"\n",
        "\n",
        "        # set self.model\n",
        "        self.model = model\n",
        "\n",
        "        # set category_mapping\n",
        "        if not self.category_mapping:\n",
        "            category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n",
        "            self.category_mapping = category_mapping\n",
        "\n",
        "    def perform_inference(self, image: np.ndarray):\n",
        "        \"\"\"\n",
        "        Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n",
        "        Args:\n",
        "            image: np.ndarray\n",
        "                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n",
        "        \"\"\"\n",
        "        check_requirements([\"torch\", \"mmdet\", \"mmcv\"])\n",
        "\n",
        "        # Confirm model is loaded\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n",
        "        # Supports only batch of 1\n",
        "        from mmdet.apis import inference_detector\n",
        "\n",
        "        # perform inference\n",
        "        if isinstance(image, np.ndarray):\n",
        "            # https://github.com/obss/sahi/issues/265\n",
        "            image = image[:, :, ::-1]\n",
        "        # compatibility with sahi v0.8.15\n",
        "        if not isinstance(image, list):\n",
        "            image = [image]\n",
        "        prediction_result = inference_detector(self.model, image)\n",
        "\n",
        "        self._original_predictions = prediction_result\n",
        "\n",
        "    @property\n",
        "    def num_categories(self):\n",
        "        \"\"\"\n",
        "        Returns number of categories\n",
        "        \"\"\"\n",
        "        if isinstance(self.model.CLASSES, str):\n",
        "            num_categories = 1\n",
        "        else:\n",
        "            num_categories = len(self.model.CLASSES)\n",
        "        return num_categories\n",
        "\n",
        "    @property\n",
        "    def has_mask(self):\n",
        "        \"\"\"\n",
        "        Returns if model output contains segmentation mask\n",
        "        \"\"\"\n",
        "        has_mask = self.model.with_mask\n",
        "        return has_mask\n",
        "\n",
        "    @property\n",
        "    def category_names(self):\n",
        "        if type(self.model.CLASSES) == str:\n",
        "            # https://github.com/open-mmlab/mmdetection/pull/4973\n",
        "            return (self.model.CLASSES,)\n",
        "        else:\n",
        "            return self.model.CLASSES\n",
        "\n",
        "    def _create_object_prediction_list_from_original_predictions(\n",
        "        self,\n",
        "        shift_amount_list: Optional[List[List[int]]] = [[0, 0]],\n",
        "        full_shape_list: Optional[List[List[int]]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n",
        "        self._object_prediction_list_per_image.\n",
        "        Args:\n",
        "            shift_amount_list: list of list\n",
        "                To shift the box and mask predictions from sliced image to full sized image, should\n",
        "                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n",
        "            full_shape_list: list of list\n",
        "                Size of the full image after shifting, should be in the form of\n",
        "                List[[height, width],[height, width],...]\n",
        "        \"\"\"\n",
        "        original_predictions = self._original_predictions\n",
        "        category_mapping = self.category_mapping\n",
        "\n",
        "        # compatilibty for sahi v0.8.15\n",
        "        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n",
        "        full_shape_list = fix_full_shape_list(full_shape_list)\n",
        "\n",
        "        # parse boxes and masks from predictions\n",
        "        num_categories = self.num_categories\n",
        "        object_prediction_list_per_image = []\n",
        "        for image_ind, original_prediction in enumerate(original_predictions):\n",
        "            shift_amount = shift_amount_list[image_ind]\n",
        "            full_shape = None if full_shape_list is None else full_shape_list[image_ind]\n",
        "\n",
        "            if self.has_mask:\n",
        "                boxes = original_prediction[0]\n",
        "                masks = original_prediction[1]\n",
        "            else:\n",
        "                boxes = original_prediction\n",
        "\n",
        "            object_prediction_list = []\n",
        "\n",
        "            # process predictions\n",
        "            for category_id in range(num_categories):\n",
        "                category_boxes = boxes[category_id]\n",
        "                if self.has_mask:\n",
        "                    category_masks = masks[category_id]\n",
        "                num_category_predictions = len(category_boxes)\n",
        "\n",
        "                for category_predictions_ind in range(num_category_predictions):\n",
        "                    bbox = category_boxes[category_predictions_ind][:4]\n",
        "                    score = category_boxes[category_predictions_ind][4]\n",
        "                    category_name = category_mapping[str(category_id)]\n",
        "\n",
        "                    # ignore low scored predictions\n",
        "                    if score < self.confidence_threshold:\n",
        "                        continue\n",
        "\n",
        "                    # parse prediction mask\n",
        "                    if self.has_mask:\n",
        "                        bool_mask = category_masks[category_predictions_ind]\n",
        "                        # check if mask is valid\n",
        "                        # https://github.com/obss/sahi/issues/389\n",
        "                        if get_bbox_from_bool_mask(bool_mask) is None:\n",
        "                            continue\n",
        "                    else:\n",
        "                        bool_mask = None\n",
        "\n",
        "                    # fix negative box coords\n",
        "                    bbox[0] = max(0, bbox[0])\n",
        "                    bbox[1] = max(0, bbox[1])\n",
        "                    bbox[2] = max(0, bbox[2])\n",
        "                    bbox[3] = max(0, bbox[3])\n",
        "\n",
        "                    # fix out of image box coords\n",
        "                    if full_shape is not None:\n",
        "                        bbox[0] = min(full_shape[1], bbox[0])\n",
        "                        bbox[1] = min(full_shape[0], bbox[1])\n",
        "                        bbox[2] = min(full_shape[1], bbox[2])\n",
        "                        bbox[3] = min(full_shape[0], bbox[3])\n",
        "\n",
        "                    # ignore invalid predictions\n",
        "                    if not (bbox[0] < bbox[2]) or not (bbox[1] < bbox[3]):\n",
        "                        logger.warning(f\"ignoring invalid prediction with bbox: {bbox}\")\n",
        "                        continue\n",
        "\n",
        "                    object_prediction = ObjectPrediction(\n",
        "                        bbox=bbox,\n",
        "                        category_id=category_id,\n",
        "                        score=score,\n",
        "                        bool_mask=bool_mask,\n",
        "                        category_name=category_name,\n",
        "                        shift_amount=shift_amount,\n",
        "                        full_shape=full_shape,\n",
        "                    )\n",
        "                    object_prediction_list.append(object_prediction)\n",
        "            object_prediction_list_per_image.append(object_prediction_list)\n",
        "        self._object_prediction_list_per_image = object_prediction_list_per_image\n",
        "\n",
        "\n",
        "class Yolov5DetectionModel(DetectionModel):\n",
        "    def load_model(self):\n",
        "        \"\"\"\n",
        "        Detection model is initialized and set to self.model.\n",
        "        \"\"\"\n",
        "        check_requirements([\"torch\", \"yolov5\"])\n",
        "\n",
        "        import yolov5\n",
        "\n",
        "        try:\n",
        "            model = yolov5.load(self.model_path, device=self.device)\n",
        "            self.set_model(model)\n",
        "        except Exception as e:\n",
        "            raise TypeError(\"model_path is not a valid yolov5 model path: \", e)\n",
        "\n",
        "    def set_model(self, model: Any):\n",
        "        \"\"\"\n",
        "        Sets the underlying YOLOv5 model.\n",
        "        Args:\n",
        "            model: Any\n",
        "                A YOLOv5 model\n",
        "        \"\"\"\n",
        "\n",
        "        if model.__class__.__module__ not in [\"yolov5.models.common\", \"models.common\"]:\n",
        "            raise Exception(f\"Not a yolov5 model: {type(model)}\")\n",
        "\n",
        "        model.conf = self.confidence_threshold\n",
        "        self.model = model\n",
        "\n",
        "        # set category_mapping\n",
        "        if not self.category_mapping:\n",
        "            category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n",
        "            self.category_mapping = category_mapping\n",
        "\n",
        "    def perform_inference(self, image: np.ndarray):\n",
        "        \"\"\"\n",
        "        Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n",
        "        Args:\n",
        "            image: np.ndarray\n",
        "                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n",
        "        \"\"\"\n",
        "\n",
        "        # Confirm model is loaded\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n",
        "        if self.image_size is not None:\n",
        "            prediction_result = self.model(image, size=self.image_size)\n",
        "        else:\n",
        "            prediction_result = self.model(image)\n",
        "\n",
        "        self._original_predictions = prediction_result\n",
        "\n",
        "    @property\n",
        "    def num_categories(self):\n",
        "        \"\"\"\n",
        "        Returns number of categories\n",
        "        \"\"\"\n",
        "        return len(self.model.names)\n",
        "\n",
        "    @property\n",
        "    def has_mask(self):\n",
        "        \"\"\"\n",
        "        Returns if model output contains segmentation mask\n",
        "        \"\"\"\n",
        "        import yolov5\n",
        "        from packaging import version\n",
        "\n",
        "        if version.parse(yolov5.__version__) < version.parse(\"6.2.0\"):\n",
        "            return False\n",
        "        else:\n",
        "            return False  # fix when yolov5 supports segmentation models\n",
        "\n",
        "    @property\n",
        "    def category_names(self):\n",
        "        import yolov5\n",
        "        from packaging import version\n",
        "\n",
        "        if version.parse(yolov5.__version__) >= version.parse(\"6.2.0\"):\n",
        "            return list(self.model.names.values())\n",
        "        else:\n",
        "            return self.model.names\n",
        "\n",
        "    def _create_object_prediction_list_from_original_predictions(\n",
        "        self,\n",
        "        shift_amount_list: Optional[List[List[int]]] = [[0, 0]],\n",
        "        full_shape_list: Optional[List[List[int]]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n",
        "        self._object_prediction_list_per_image.\n",
        "        Args:\n",
        "            shift_amount_list: list of list\n",
        "                To shift the box and mask predictions from sliced image to full sized image, should\n",
        "                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n",
        "            full_shape_list: list of list\n",
        "                Size of the full image after shifting, should be in the form of\n",
        "                List[[height, width],[height, width],...]\n",
        "        \"\"\"\n",
        "        original_predictions = self._original_predictions\n",
        "\n",
        "        # compatilibty for sahi v0.8.15\n",
        "        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n",
        "        full_shape_list = fix_full_shape_list(full_shape_list)\n",
        "\n",
        "        # handle all predictions\n",
        "        object_prediction_list_per_image = []\n",
        "        for image_ind, image_predictions_in_xyxy_format in enumerate(original_predictions.xyxy):\n",
        "            shift_amount = shift_amount_list[image_ind]\n",
        "            full_shape = None if full_shape_list is None else full_shape_list[image_ind]\n",
        "            object_prediction_list = []\n",
        "\n",
        "            # process predictions\n",
        "            for prediction in image_predictions_in_xyxy_format.cpu().detach().numpy():\n",
        "                x1 = int(prediction[0])\n",
        "                y1 = int(prediction[1])\n",
        "                x2 = int(prediction[2])\n",
        "                y2 = int(prediction[3])\n",
        "                bbox = [x1, y1, x2, y2]\n",
        "                score = prediction[4]\n",
        "                category_id = int(prediction[5])\n",
        "                category_name = self.category_mapping[str(category_id)]\n",
        "\n",
        "                # fix negative box coords\n",
        "                bbox[0] = max(0, bbox[0])\n",
        "                bbox[1] = max(0, bbox[1])\n",
        "                bbox[2] = max(0, bbox[2])\n",
        "                bbox[3] = max(0, bbox[3])\n",
        "\n",
        "                # fix out of image box coords\n",
        "                if full_shape is not None:\n",
        "                    bbox[0] = min(full_shape[1], bbox[0])\n",
        "                    bbox[1] = min(full_shape[0], bbox[1])\n",
        "                    bbox[2] = min(full_shape[1], bbox[2])\n",
        "                    bbox[3] = min(full_shape[0], bbox[3])\n",
        "\n",
        "                # ignore invalid predictions\n",
        "                if not (bbox[0] < bbox[2]) or not (bbox[1] < bbox[3]):\n",
        "                    logger.warning(f\"ignoring invalid prediction with bbox: {bbox}\")\n",
        "                    continue\n",
        "\n",
        "                object_prediction = ObjectPrediction(\n",
        "                    bbox=bbox,\n",
        "                    category_id=category_id,\n",
        "                    score=score,\n",
        "                    bool_mask=None,\n",
        "                    category_name=category_name,\n",
        "                    shift_amount=shift_amount,\n",
        "                    full_shape=full_shape,\n",
        "                )\n",
        "                object_prediction_list.append(object_prediction)\n",
        "            object_prediction_list_per_image.append(object_prediction_list)\n",
        "\n",
        "        self._object_prediction_list_per_image = object_prediction_list_per_image\n",
        "\n",
        "\n",
        "class Detectron2DetectionModel(DetectionModel):\n",
        "    def load_model(self):\n",
        "        check_requirements([\"torch\", \"detectron2\"])\n",
        "\n",
        "        from detectron2.config import get_cfg\n",
        "        from detectron2.data import MetadataCatalog\n",
        "        from detectron2.engine import DefaultPredictor\n",
        "        from detectron2.model_zoo import model_zoo\n",
        "\n",
        "        cfg = get_cfg()\n",
        "\n",
        "        try:  # try to load from model zoo\n",
        "            config_file = model_zoo.get_config_file(self.config_path)\n",
        "            cfg.merge_from_file(config_file)\n",
        "            cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(self.config_path)\n",
        "        except Exception as e:  # try to load from local\n",
        "            print(e)\n",
        "            if self.config_path is not None:\n",
        "                cfg.merge_from_file(self.config_path)\n",
        "            cfg.MODEL.WEIGHTS = self.model_path\n",
        "\n",
        "        # set model device\n",
        "        cfg.MODEL.DEVICE = self.device\n",
        "        # set input image size\n",
        "        if self.image_size is not None:\n",
        "            cfg.INPUT.MIN_SIZE_TEST = self.image_size\n",
        "            cfg.INPUT.MAX_SIZE_TEST = self.image_size\n",
        "        # init predictor\n",
        "        model = DefaultPredictor(cfg)\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        # detectron2 category mapping\n",
        "        if self.category_mapping is None:\n",
        "            try:  # try to parse category names from metadata\n",
        "                metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "                category_names = metadata.thing_classes\n",
        "                self.category_names = category_names\n",
        "                self.category_mapping = {\n",
        "                    str(ind): category_name for ind, category_name in enumerate(self.category_names)\n",
        "                }\n",
        "            except Exception as e:\n",
        "                logger.warning(e)\n",
        "                # https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html#update-the-config-for-new-datasets\n",
        "                if cfg.MODEL.META_ARCHITECTURE == \"RetinaNet\":\n",
        "                    num_categories = cfg.MODEL.RETINANET.NUM_CLASSES\n",
        "                else:  # fasterrcnn/maskrcnn etc\n",
        "                    num_categories = cfg.MODEL.ROI_HEADS.NUM_CLASSES\n",
        "                self.category_names = [str(category_id) for category_id in range(num_categories)]\n",
        "                self.category_mapping = {\n",
        "                    str(ind): category_name for ind, category_name in enumerate(self.category_names)\n",
        "                }\n",
        "        else:\n",
        "            self.category_names = list(self.category_mapping.values())\n",
        "\n",
        "    def perform_inference(self, image: np.ndarray):\n",
        "        \"\"\"\n",
        "        Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n",
        "        Args:\n",
        "            image: np.ndarray\n",
        "                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n",
        "        \"\"\"\n",
        "\n",
        "        # Confirm model is loaded\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"Model is not loaded, load it by calling .load_model()\")\n",
        "\n",
        "        if isinstance(image, np.ndarray) and self.model.input_format == \"BGR\":\n",
        "            # convert RGB image to BGR format\n",
        "            image = image[:, :, ::-1]\n",
        "\n",
        "        prediction_result = self.model(image)\n",
        "\n",
        "        self._original_predictions = prediction_result\n",
        "\n",
        "    @property\n",
        "    def num_categories(self):\n",
        "        \"\"\"\n",
        "        Returns number of categories\n",
        "        \"\"\"\n",
        "        num_categories = len(self.category_mapping)\n",
        "        return num_categories\n",
        "\n",
        "    def _create_object_prediction_list_from_original_predictions(\n",
        "        self,\n",
        "        shift_amount_list: Optional[List[List[int]]] = [[0, 0]],\n",
        "        full_shape_list: Optional[List[List[int]]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n",
        "        self._object_prediction_list_per_image.\n",
        "        Args:\n",
        "            shift_amount_list: list of list\n",
        "                To shift the box and mask predictions from sliced image to full sized image, should\n",
        "                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n",
        "            full_shape_list: list of list\n",
        "                Size of the full image after shifting, should be in the form of\n",
        "                List[[height, width],[height, width],...]\n",
        "        \"\"\"\n",
        "        original_predictions = self._original_predictions\n",
        "\n",
        "        # compatilibty for sahi v0.8.15\n",
        "        if isinstance(shift_amount_list[0], int):\n",
        "            shift_amount_list = [shift_amount_list]\n",
        "        if full_shape_list is not None and isinstance(full_shape_list[0], int):\n",
        "            full_shape_list = [full_shape_list]\n",
        "\n",
        "        # parse boxes, masks, scores, category_ids from predictions\n",
        "        boxes = original_predictions[\"instances\"].pred_boxes.tensor.tolist()\n",
        "        scores = original_predictions[\"instances\"].scores.tolist()\n",
        "        category_ids = original_predictions[\"instances\"].pred_classes.tolist()\n",
        "\n",
        "        # check if predictions contain mask\n",
        "        try:\n",
        "            masks = original_predictions[\"instances\"].pred_masks.tolist()\n",
        "        except AttributeError:\n",
        "            masks = None\n",
        "\n",
        "        # create object_prediction_list\n",
        "        object_prediction_list_per_image = []\n",
        "        object_prediction_list = []\n",
        "\n",
        "        # detectron2 DefaultPredictor supports single image\n",
        "        shift_amount = shift_amount_list[0]\n",
        "        full_shape = None if full_shape_list is None else full_shape_list[0]\n",
        "\n",
        "        for ind in range(len(boxes)):\n",
        "            score = scores[ind]\n",
        "            if score < self.confidence_threshold:\n",
        "                continue\n",
        "\n",
        "            category_id = category_ids[ind]\n",
        "\n",
        "            if masks is None:\n",
        "                bbox = boxes[ind]\n",
        "                mask = None\n",
        "            else:\n",
        "                mask = np.array(masks[ind])\n",
        "\n",
        "                # check if mask is valid\n",
        "                # https://github.com/obss/sahi/issues/389\n",
        "                if get_bbox_from_bool_mask(mask) is None:\n",
        "                    continue\n",
        "                else:\n",
        "                    bbox = None\n",
        "\n",
        "            object_prediction = ObjectPrediction(\n",
        "                bbox=bbox,\n",
        "                bool_mask=mask,\n",
        "                category_id=category_id,\n",
        "                category_name=self.category_mapping[str(category_id)],\n",
        "                shift_amount=shift_amount,\n",
        "                score=score,\n",
        "                full_shape=full_shape,\n",
        "            )\n",
        "            object_prediction_list.append(object_prediction)\n",
        "\n",
        "        # detectron2 DefaultPredictor supports single image\n",
        "        object_prediction_list_per_image = [object_prediction_list]\n",
        "\n",
        "        self._object_prediction_list_per_image = object_prediction_list_per_image\n",
        "\n",
        "\n",
        "class HuggingfaceDetectionModel(DetectionModel):\n",
        "    import torch\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: Optional[str] = None,\n",
        "        model: Optional[Any] = None,\n",
        "        feature_extractor: Optional[Any] = None,\n",
        "        config_path: Optional[str] = None,\n",
        "        device: Optional[str] = None,\n",
        "        mask_threshold: float = 0.5,\n",
        "        confidence_threshold: float = 0.3,\n",
        "        category_mapping: Optional[Dict] = None,\n",
        "        category_remapping: Optional[Dict] = None,\n",
        "        load_at_init: bool = True,\n",
        "        image_size: int = None,\n",
        "    ):\n",
        "        check_requirements([\"torch\", \"transformers\"])\n",
        "\n",
        "        self._feature_extractor = feature_extractor\n",
        "        self._image_shapes = []\n",
        "        super().__init__(\n",
        "            model_path,\n",
        "            model,\n",
        "            config_path,\n",
        "            device,\n",
        "            mask_threshold,\n",
        "            confidence_threshold,\n",
        "            category_mapping,\n",
        "            category_remapping,\n",
        "            load_at_init,\n",
        "            image_size,\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def feature_extractor(self):\n",
        "        return self._feature_extractor\n",
        "\n",
        "    @property\n",
        "    def image_shapes(self):\n",
        "        return self._image_shapes\n",
        "\n",
        "    @property\n",
        "    def num_categories(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns number of categories\n",
        "        \"\"\"\n",
        "        return self.model.config.num_labels\n",
        "\n",
        "    def load_model(self):\n",
        "        check_requirements([\"torch\", \"transformers\"])\n",
        "\n",
        "        from transformers import AutoFeatureExtractor, AutoModelForObjectDetection\n",
        "\n",
        "        model = AutoModelForObjectDetection.from_pretrained(self.model_path)\n",
        "        if self.image_size is not None:\n",
        "            feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
        "                self.model_path, size=self.image_size, do_resize=True\n",
        "            )\n",
        "        else:\n",
        "            feature_extractor = AutoFeatureExtractor.from_pretrained(self.model_path)\n",
        "        self.set_model(model, feature_extractor)\n",
        "\n",
        "    def set_model(self, model: Any, feature_extractor: Any = None):\n",
        "        feature_extractor = feature_extractor or self.feature_extractor\n",
        "        if feature_extractor is None:\n",
        "            raise ValueError(f\"'feature_extractor' is required to be set, got {feature_extractor}.\")\n",
        "        elif (\n",
        "            \"ObjectDetection\" not in model.__class__.__name__\n",
        "            or \"FeatureExtractor\" not in feature_extractor.__class__.__name__\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                f\"Given 'model' is not an ObjectDetectionModel or 'feature_extractor' is not a valid FeatureExtractor.\"\n",
        "            )\n",
        "        self.model = model\n",
        "        self.model.to(self.device)\n",
        "        self._feature_extractor = feature_extractor\n",
        "        self.category_mapping = self.model.config.id2label\n",
        "\n",
        "    def perform_inference(self, image: Union[List, np.ndarray]):\n",
        "        \"\"\"\n",
        "        Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n",
        "        Args:\n",
        "            image: np.ndarray\n",
        "                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n",
        "        \"\"\"\n",
        "        import torch\n",
        "\n",
        "        # Confirm model is loaded\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"Model is not loaded, load it by calling .load_model()\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
        "            inputs[\"pixel_values\"] = inputs.pixel_values.to(self.device)\n",
        "            if hasattr(inputs, \"pixel_mask\"):\n",
        "                inputs[\"pixel_mask\"] = inputs.pixel_mask.to(self.device)\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        if isinstance(image, list):\n",
        "            self._image_shapes = [img.shape for img in image]\n",
        "        else:\n",
        "            self._image_shapes = [image.shape]\n",
        "        self._original_predictions = outputs\n",
        "\n",
        "    def get_valid_predictions(\n",
        "        self, logits: torch.Tensor, pred_boxes: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        import torch\n",
        "\n",
        "        probs = logits.softmax(-1)\n",
        "        scores = probs.max(-1).values\n",
        "        cat_ids = probs.argmax(-1)\n",
        "        valid_detections = torch.where(cat_ids < self.num_categories, 1, 0)\n",
        "        valid_confidences = torch.where(scores >= self.confidence_threshold, 1, 0)\n",
        "        valid_mask = valid_detections.logical_and(valid_confidences)\n",
        "        scores = scores[valid_mask]\n",
        "        cat_ids = cat_ids[valid_mask]\n",
        "        boxes = pred_boxes[valid_mask]\n",
        "        return scores, cat_ids, boxes\n",
        "\n",
        "    def _create_object_prediction_list_from_original_predictions(\n",
        "        self,\n",
        "        shift_amount_list: Optional[List[List[int]]] = [[0, 0]],\n",
        "        full_shape_list: Optional[List[List[int]]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n",
        "        self._object_prediction_list_per_image.\n",
        "        Args:\n",
        "            shift_amount_list: list of list\n",
        "                To shift the box and mask predictions from sliced image to full sized image, should\n",
        "                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n",
        "            full_shape_list: list of list\n",
        "                Size of the full image after shifting, should be in the form of\n",
        "                List[[height, width],[height, width],...]\n",
        "        \"\"\"\n",
        "        original_predictions = self._original_predictions\n",
        "\n",
        "        # compatilibty for sahi v0.8.15\n",
        "        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n",
        "        full_shape_list = fix_full_shape_list(full_shape_list)\n",
        "\n",
        "        n_image = original_predictions.logits.shape[0]\n",
        "        object_prediction_list_per_image = []\n",
        "        for image_ind in range(n_image):\n",
        "            image_height, image_width, _ = self.image_shapes[image_ind]\n",
        "            scores, cat_ids, boxes = self.get_valid_predictions(\n",
        "                logits=original_predictions.logits[image_ind], pred_boxes=original_predictions.pred_boxes[image_ind]\n",
        "            )\n",
        "\n",
        "            # create object_prediction_list\n",
        "            object_prediction_list = []\n",
        "\n",
        "            shift_amount = shift_amount_list[image_ind]\n",
        "            full_shape = None if full_shape_list is None else full_shape_list[image_ind]\n",
        "\n",
        "            for ind in range(len(boxes)):\n",
        "                category_id = cat_ids[ind].item()\n",
        "                yolo_bbox = boxes[ind].tolist()\n",
        "                bbox = list(\n",
        "                    pbf.convert_bbox(\n",
        "                        yolo_bbox,\n",
        "                        from_type=\"yolo\",\n",
        "                        to_type=\"voc\",\n",
        "                        image_size=(image_width, image_height),\n",
        "                        return_values=True,\n",
        "                        strict=False,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                # fix negative box coords\n",
        "                bbox[0] = max(0, int(bbox[0]))\n",
        "                bbox[1] = max(0, int(bbox[1]))\n",
        "                bbox[2] = min(bbox[2], image_width)\n",
        "                bbox[3] = min(bbox[3], image_height)\n",
        "\n",
        "                object_prediction = ObjectPrediction(\n",
        "                    bbox=bbox,\n",
        "                    bool_mask=None,\n",
        "                    category_id=category_id,\n",
        "                    category_name=self.category_mapping[category_id],\n",
        "                    shift_amount=shift_amount,\n",
        "                    score=scores[ind].item(),\n",
        "                    full_shape=full_shape,\n",
        "                )\n",
        "                object_prediction_list.append(object_prediction)\n",
        "            object_prediction_list_per_image.append(object_prediction_list)\n",
        "\n",
        "        self._object_prediction_list_per_image = object_prediction_list_per_image\n",
        "\n",
        "\n",
        "class TorchVisionDetectionModel(DetectionModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: Optional[str] = None,\n",
        "        model: Optional[Any] = None,\n",
        "        config_path: Optional[str] = None,\n",
        "        device: Optional[str] = None,\n",
        "        mask_threshold: float = 0.5,\n",
        "        confidence_threshold: float = 0.3,\n",
        "        category_mapping: Optional[Dict] = None,\n",
        "        category_remapping: Optional[Dict] = None,\n",
        "        load_at_init: bool = True,\n",
        "        image_size: int = None,\n",
        "    ):\n",
        "\n",
        "        super().__init__(\n",
        "            model_path=model_path,\n",
        "            model=model,\n",
        "            config_path=config_path,\n",
        "            device=device,\n",
        "            mask_threshold=mask_threshold,\n",
        "            confidence_threshold=confidence_threshold,\n",
        "            category_mapping=category_mapping,\n",
        "            category_remapping=category_remapping,\n",
        "            load_at_init=load_at_init,\n",
        "            image_size=image_size,\n",
        "        )\n",
        "\n",
        "    def load_model(self):\n",
        "        check_requirements([\"torch\", \"torchvision\"])\n",
        "\n",
        "        import torch\n",
        "\n",
        "        from sahi.utils.torchvision import MODEL_NAME_TO_CONSTRUCTOR\n",
        "\n",
        "        # read config params\n",
        "        model_name = None\n",
        "        num_classes = None\n",
        "        if self.config_path is not None:\n",
        "            import yaml\n",
        "\n",
        "            with open(self.config_path, \"r\") as stream:\n",
        "                try:\n",
        "                    config = yaml.safe_load(stream)\n",
        "                except yaml.YAMLError as exc:\n",
        "                    raise RuntimeError(exc)\n",
        "\n",
        "            model_name = config.get(\"model_name\", None)\n",
        "            num_classes = config.get(\"num_classes\", None)\n",
        "\n",
        "        # complete params if not provided in config\n",
        "        if not model_name:\n",
        "            model_name = \"fasterrcnn_resnet50_fpn\"\n",
        "            logger.warning(f\"model_name not provided in config, using default model_type: {model_name}'\")\n",
        "        if num_classes is None:\n",
        "            logger.warning(\"num_classes not provided in config, using default num_classes: 91\")\n",
        "            num_classes = 91\n",
        "        if self.model_path is None:\n",
        "            logger.warning(\"model_path not provided in config, using pretrained weights and default num_classes: 91.\")\n",
        "            pretrained = True\n",
        "            num_classes = 91\n",
        "        else:\n",
        "            pretrained = False\n",
        "\n",
        "        # load model\n",
        "        model = MODEL_NAME_TO_CONSTRUCTOR[model_name](num_classes=num_classes, pretrained=pretrained)\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(self.model_path))\n",
        "        except Exception as e:\n",
        "            TypeError(\"model_path is not a valid torchvision model path: \", e)\n",
        "\n",
        "        self.set_model(model)\n",
        "\n",
        "    def set_model(self, model: Any):\n",
        "        \"\"\"\n",
        "        Sets the underlying TorchVision model.\n",
        "        Args:\n",
        "            model: Any\n",
        "                A TorchVision model\n",
        "        \"\"\"\n",
        "        check_requirements([\"torch\", \"torchvision\"])\n",
        "\n",
        "        model.eval()\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "        # set category_mapping\n",
        "        from sahi.utils.torchvision import COCO_CLASSES\n",
        "\n",
        "        if self.category_mapping is None:\n",
        "            category_names = {str(i): COCO_CLASSES[i] for i in range(len(COCO_CLASSES))}\n",
        "            self.category_mapping = category_names\n",
        "\n",
        "    def perform_inference(self, image: np.ndarray, image_size: int = None):\n",
        "        \"\"\"\n",
        "        Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n",
        "        Args:\n",
        "            image: np.ndarray\n",
        "                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n",
        "            image_size: int\n",
        "                Inference input size.\n",
        "        \"\"\"\n",
        "        from sahi.utils.torch import to_float_tensor\n",
        "\n",
        "        # arrange model input size\n",
        "        if self.image_size is not None:\n",
        "            # get min and max of image height and width\n",
        "            min_shape, max_shape = min(image.shape[:2]), max(image.shape[:2])\n",
        "            # torchvision resize transform scales the shorter dimension to the target size\n",
        "            # we want to scale the longer dimension to the target size\n",
        "            image_size = self.image_size * min_shape / max_shape\n",
        "            self.model.transform.min_size = (image_size,)  # default is (800,)\n",
        "            self.model.transform.max_size = image_size  # default is 1333\n",
        "\n",
        "        image = to_float_tensor(image)\n",
        "        image = image.to(self.device)\n",
        "        prediction_result = self.model([image])\n",
        "\n",
        "        self._original_predictions = prediction_result\n",
        "\n",
        "    @property\n",
        "    def num_categories(self):\n",
        "        \"\"\"\n",
        "        Returns number of categories\n",
        "        \"\"\"\n",
        "        return len(self.category_mapping)\n",
        "\n",
        "    @property\n",
        "    def has_mask(self):\n",
        "        \"\"\"\n",
        "        Returns if model output contains segmentation mask\n",
        "        \"\"\"\n",
        "        return self.model.with_mask\n",
        "\n",
        "    @property\n",
        "    def category_names(self):\n",
        "        return list(self.category_mapping.values())\n",
        "\n",
        "    def _create_object_prediction_list_from_original_predictions(\n",
        "        self,\n",
        "        shift_amount_list: Optional[List[List[int]]] = [[0, 0]],\n",
        "        full_shape_list: Optional[List[List[int]]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n",
        "        self._object_prediction_list_per_image.\n",
        "        Args:\n",
        "            shift_amount_list: list of list\n",
        "                To shift the box and mask predictions from sliced image to full sized image, should\n",
        "                be in the form of List[[shift_x, shift_y],[shift_x, shift_y],...]\n",
        "            full_shape_list: list of list\n",
        "                Size of the full image after shifting, should be in the form of\n",
        "                List[[height, width],[height, width],...]\n",
        "        \"\"\"\n",
        "        original_predictions = self._original_predictions\n",
        "\n",
        "        # compatilibty for sahi v0.8.20\n",
        "        if isinstance(shift_amount_list[0], int):\n",
        "            shift_amount_list = [shift_amount_list]\n",
        "        if full_shape_list is not None and isinstance(full_shape_list[0], int):\n",
        "            full_shape_list = [full_shape_list]\n",
        "\n",
        "        for image_predictions in original_predictions:\n",
        "            object_prediction_list_per_image = []\n",
        "\n",
        "            # get indices of boxes with score > confidence_threshold\n",
        "            scores = image_predictions[\"scores\"].cpu().detach().numpy()\n",
        "            selected_indices = np.where(scores > self.confidence_threshold)[0]\n",
        "\n",
        "            # parse boxes, masks, scores, category_ids from predictions\n",
        "            category_ids = list(image_predictions[\"labels\"][selected_indices].cpu().detach().numpy())\n",
        "            boxes = list(image_predictions[\"boxes\"][selected_indices].cpu().detach().numpy())\n",
        "            scores = scores[selected_indices]\n",
        "\n",
        "            # check if predictions contain mask\n",
        "            masks = image_predictions.get(\"masks\", None)\n",
        "            if masks is not None:\n",
        "                masks = list(image_predictions[\"masks\"][selected_indices].cpu().detach().numpy())\n",
        "            else:\n",
        "                masks = None\n",
        "\n",
        "            # create object_prediction_list\n",
        "            object_prediction_list = []\n",
        "\n",
        "            shift_amount = shift_amount_list[0]\n",
        "            full_shape = None if full_shape_list is None else full_shape_list[0]\n",
        "\n",
        "            for ind in range(len(boxes)):\n",
        "\n",
        "                if masks is not None:\n",
        "                    mask = np.array(masks[ind])\n",
        "                else:\n",
        "                    mask = None\n",
        "\n",
        "                object_prediction = ObjectPrediction(\n",
        "                    bbox=boxes[ind],\n",
        "                    bool_mask=mask,\n",
        "                    category_id=int(category_ids[ind]),\n",
        "                    category_name=self.category_mapping[str(int(category_ids[ind]))],\n",
        "                    shift_amount=shift_amount,\n",
        "                    score=scores[ind],\n",
        "                    full_shape=full_shape,\n",
        "                )\n",
        "                object_prediction_list.append(object_prediction)\n",
        "            object_prediction_list_per_image.append(object_prediction_list)\n",
        "\n",
        "        self._object_prediction_list_per_image = object_prediction_list_per_image\n",
        "\n",
        "\n",
        "class Yolov7DetectionModel(DetectionModel):\n",
        "    def load_model(self):\n",
        "        check_requirements([\"torch\"])\n",
        "        import torch\n",
        "\n",
        "        try:\n",
        "            model = torch.hub.load(\"WongKinYiu/yolov7\", \"custom\", self.model_path)\n",
        "            model.conf = self.confidence_threshold\n",
        "            self.model = model\n",
        "        except ImportError:\n",
        "            raise ImportError(\n",
        "                'Please run \"pip install -r https://raw.githubusercontent.com/WongKinYiu/yolov7/main/requirements.txt\" '\n",
        "                \"to install Yolov7 first for Yolov7 inference.\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            TypeError(\"model_path is not a valid torchvision model path: \", e)\n",
        "\n",
        "    def perform_inference(self, image: np.ndarray):\n",
        "        \"\"\"\n",
        "        Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n",
        "        Args:\n",
        "            image: np.ndarray\n",
        "                A numpy array that contains the image to be predicted. 3 channel image should be in RGB order.\n",
        "        \"\"\"\n",
        "        check_requirements([\"torch\"])\n",
        "        # Confirm model is loaded\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model is not loaded, load it by calling .load_model()\")\n",
        "        if self.image_size is not None:\n",
        "            prediction_result = self.model(image, size=self.image_size)\n",
        "        else:\n",
        "            prediction_result = self.model(image)\n",
        "\n",
        "        self._original_predictions = prediction_result\n",
        "\n",
        "    def _create_object_prediction_list_from_original_predictions(\n",
        "        self,\n",
        "        shift_amount_list: Optional[List[List[int]]] = [[0, 0]],\n",
        "        full_shape_list: Optional[List[List[int]]] = None,\n",
        "    ):\n",
        "        # compatilibty for sahi v0.8.20\n",
        "        if isinstance(shift_amount_list[0], int):\n",
        "            shift_amount_list = [shift_amount_list]\n",
        "        if full_shape_list is not None and isinstance(full_shape_list[0], int):\n",
        "            full_shape_list = [full_shape_list]\n",
        "\n",
        "        shift_amount = shift_amount_list[0]\n",
        "        full_shape = None if full_shape_list is None else full_shape_list[0]\n",
        "\n",
        "        object_prediction_list_per_image = []\n",
        "        object_prediction_list = []\n",
        "        for _, image_predictions_in_xyxy_format in enumerate(self._original_predictions.xyxy):\n",
        "            for pred in image_predictions_in_xyxy_format.cpu().detach().numpy():\n",
        "                x1, y1, x2, y2 = (\n",
        "                    int(pred[0]),\n",
        "                    int(pred[1]),\n",
        "                    int(pred[2]),\n",
        "                    int(pred[3]),\n",
        "                )\n",
        "                bbox = [x1, y1, x2, y2]\n",
        "                score = pred[4]\n",
        "                category_name = self.model.names[int(pred[5])]\n",
        "                category_id = pred[5]\n",
        "                object_prediction = ObjectPrediction(\n",
        "                    bbox=bbox,\n",
        "                    category_id=int(category_id),\n",
        "                    score=score,\n",
        "                    bool_mask=None,\n",
        "                    category_name=category_name,\n",
        "                    shift_amount=shift_amount,\n",
        "                    full_shape=full_shape,\n",
        "                )\n",
        "                object_prediction_list.append(object_prediction)\n",
        "            object_prediction_list_per_image.append(object_prediction_list)\n",
        "\n",
        "        self._object_prediction_list_per_image = object_prediction_list_per_image"
      ],
      "metadata": {
        "id": "JwfWnt17205T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from os import path\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "class Yolov7TestConstants:\n",
        "    YOLOV7_MODEL_URL = \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\"\n",
        "    YOLOV7_MODEL_PATH = \"tests/data/models/yolov7/yolov7.pt\"\n",
        "\n",
        "    YOLOV7_TINY_MODEL_URL = \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\"\n",
        "    YOLOV7_TINY_MODEL_PATH = \"tests/data/models/yolov7/yolov7-tiny.pt\"\n",
        "\n",
        "    YOLOV7_E6_MODEL_URL = \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6.pt\"\n",
        "    YOLOV7_E6_MODEL_PATH = \"tests/data/models/yolov7/yolov7-e6.pt\"\n",
        "\n",
        "\n",
        "def download_yolov7_model(destination_path: Optional[str] = None):\n",
        "\n",
        "    if destination_path is None:\n",
        "        destination_path = Yolov7TestConstants.YOLOV7_MODEL_PATH\n",
        "\n",
        "    Path(destination_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if not path.exists(destination_path):\n",
        "        urllib.request.urlretrieve(\n",
        "            Yolov7TestConstants.YOLOV7_MODEL_URL,\n",
        "            destination_path,\n",
        "        )\n",
        "\n",
        "\n",
        "def download_yolov7e6_model(destination_path: Optional[str] = None):\n",
        "\n",
        "    if destination_path is None:\n",
        "        destination_path = Yolov7TestConstants.YOLOV7_E6_MODEL_PATH\n",
        "\n",
        "    Path(destination_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if not path.exists(destination_path):\n",
        "        urllib.request.urlretrieve(\n",
        "            Yolov7TestConstants.YOLOV7_E6_MODEL_URL,\n",
        "            destination_path,\n",
        "        )"
      ],
      "metadata": {
        "id": "kTZBBRm93R9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download yolov7 model to 'models/yolov7.pt'\n",
        "yolov7_model_path = '/content/gdrive/MyDrive/yolov7/runs/train/exp32/weights/best.pt'\n",
        "download_yolov7_model(destination_path=yolov7_model_path)"
      ],
      "metadata": {
        "id": "MxppOIsW23GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detection_model = Yolov7DetectionModel(\n",
        "    model_path=yolov7_model_path,\n",
        "    confidence_threshold=0.01,\n",
        "    image_size=928,\n",
        "    device='0'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4JW2VdV24EY",
        "outputId": "cbb65c3c-e61b-48ae-c30f-85c35f437709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/WongKinYiu/yolov7/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding autoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "\n",
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "WVrPJSwzFs-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start streaming video from webcam\n",
        "import IPython\n",
        "from IPython.display import Image\n",
        "import math\n",
        "from PIL import Image\n",
        "import datetime\n",
        "import numpy as np\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  nk=1\n",
        "  prev=datetime.datetime.now()\n",
        "\n",
        "  while True:\n",
        "    count=0\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "    cur= datetime.datetime.now()\n",
        "    if ( (math.floor(((cur-prev).total_seconds())/60) !=2)  and  nk!=1):\n",
        "        continue\n",
        "\n",
        "    nk=1\n",
        "    img0 = js_to_image(js_reply[\"img\"])\n",
        "    result = get_sliced_prediction(\n",
        "        img0,\n",
        "        detection_model,\n",
        "        slice_height = 288,\n",
        "        slice_width = 512,\n",
        "        overlap_height_ratio = 0.2,\n",
        "        overlap_width_ratio = 0.2,\n",
        "        )\n",
        "    object_prediction_list = result.object_prediction_list\n",
        "    count =len(object_prediction_list)\n",
        "    result.export_visuals(export_dir=\"/content/\")\n",
        "\n",
        "\n",
        "    im = Image.open(\"/content/prediction_visual.png\")\n",
        "    #display(im)\n",
        "\n",
        "    cur= datetime.datetime.now()\n",
        "    if count>20 :\n",
        "        nk=2\n",
        "        prev=datetime.datetime.now()\n",
        "        display(IPython.display.Audio(url=\"https://cdn.pixabay.com/download/audio/2022/01/18/audio_86f38dcf0e.mp3?filename=attack2t22wav-14511.mp3\", autoplay=True))\n",
        "        print(count)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        },
        "id": "Uv41lHfX2Ska",
        "outputId": "e3bca689-6d87-4c5d-da86-283c29c4383c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 4 number of slices.\n",
            "Performing prediction on 4 number of slices.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
              "                    <source src=\"https://cdn.pixabay.com/download/audio/2022/01/18/audio_86f38dcf0e.mp3?filename=attack2t22wav-14511.mp3\" type=\"audio/mpeg\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6e33df9b3c16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mjs_reply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjs_reply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-13907c905f78>\u001b[0m in \u001b[0;36mvideo_frame\u001b[0;34m(label, bbox)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stream_frame(\"{}\", \"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#give the full path to video, your video will be in the Yolov7 folder\n",
        "video_path = '/content/gdrive/MyDrive/d3.mp4'"
      ],
      "metadata": {
        "id": "fvJLGgf0LVYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing video object\n",
        "video = cv2.VideoCapture(video_path)\n",
        "import datetime\n",
        "import numpy\n",
        "import math\n",
        "import IPython\n",
        "from IPython.display import Image\n",
        "import math\n",
        "from PIL import Image\n",
        "import datetime\n",
        "import numpy as np\n",
        "import cv2\n",
        "#Video information\n",
        "fps = video.get(cv2.CAP_PROP_FPS)\n",
        "w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "nframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Initialzing object for writing video output\n",
        "output = cv2.VideoWriter('output1.avi', cv2.VideoWriter_fourcc(*'MJPG'),fps , (w,h))\n",
        "torch.cuda.empty_cache()\n",
        "# Initializing model and setting it for inference\n",
        "with torch.no_grad():\n",
        "\n",
        "  k=0\n",
        "  count=0\n",
        "  cur=datetime.datetime.now()\n",
        "  for j in range(nframes):\n",
        "      k=k+1\n",
        "\n",
        "      ret, img0 = video.read()\n",
        "      if ret:\n",
        "        img0=cv2.resize(img0,None,fx=1.5,fy=1.5,interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        '''result = get_sliced_prediction(\n",
        "        img0,\n",
        "        detection_model,\n",
        "        slice_height = 288,\n",
        "        slice_width = 512,\n",
        "        overlap_height_ratio = 0.2,\n",
        "        overlap_width_ratio = 0.2,\n",
        "        postprocess_type ='NMS',\n",
        "        auto_slice_resolution =True,\n",
        "        verbose=1\n",
        "\n",
        "        )'''\n",
        "        result = get_prediction(img0,detection_model)\n",
        "        object_prediction_list = result.object_prediction_list\n",
        "\n",
        "        count =count+len(object_prediction_list)\n",
        "        print(len(object_prediction_list))\n",
        "        result.export_visuals(export_dir=\"/content/\")\n",
        "\n",
        "        im=cv2.imread(\"/content/prediction_visual.png\", 1)\n",
        "        #im=cv2.resize(im,None,fx=0.5,fy=0.5,interpolation=cv2.INTER_CUBIC)\n",
        "        output.write(numpy.asarray(im))\n",
        "      else:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "output.release()\n",
        "video.release()\n",
        "now=datetime.datetime.now()\n",
        "print(122/(now-cur).total_seconds())\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djSEWic8MHPa",
        "outputId": "a84c6cdf-f5f1-4ae1-e0f7-6cf2a9343a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88\n",
            "84\n",
            "84\n",
            "80\n",
            "82\n",
            "87\n",
            "94\n",
            "92\n",
            "89\n",
            "89\n",
            "93\n",
            "92\n",
            "92\n",
            "92\n",
            "94\n",
            "94\n",
            "98\n",
            "96\n",
            "89\n",
            "88\n",
            "88\n",
            "93\n",
            "91\n",
            "90\n",
            "93\n",
            "94\n",
            "96\n",
            "95\n",
            "96\n",
            "93\n",
            "92\n",
            "94\n",
            "95\n",
            "92\n",
            "96\n",
            "90\n",
            "88\n",
            "87\n",
            "88\n",
            "96\n",
            "93\n",
            "91\n",
            "90\n",
            "86\n",
            "96\n",
            "95\n",
            "92\n",
            "94\n",
            "99\n",
            "90\n",
            "98\n",
            "108\n",
            "110\n",
            "100\n",
            "105\n",
            "97\n",
            "96\n",
            "98\n",
            "92\n",
            "97\n",
            "97\n",
            "95\n",
            "97\n",
            "95\n",
            "103\n",
            "102\n",
            "102\n",
            "101\n",
            "97\n",
            "101\n",
            "97\n",
            "99\n",
            "97\n",
            "104\n",
            "104\n",
            "110\n",
            "97\n",
            "95\n",
            "95\n",
            "102\n",
            "91\n",
            "87\n",
            "88\n",
            "99\n",
            "101\n",
            "103\n",
            "98\n",
            "97\n",
            "103\n",
            "109\n",
            "104\n",
            "101\n",
            "99\n",
            "98\n",
            "92\n",
            "100\n",
            "96\n",
            "97\n",
            "100\n",
            "102\n",
            "97\n",
            "93\n",
            "98\n",
            "102\n",
            "97\n",
            "100\n",
            "99\n",
            "100\n",
            "96\n",
            "106\n",
            "106\n",
            "95\n",
            "94\n",
            "99\n",
            "86\n",
            "100\n",
            "90\n",
            "88\n",
            "93\n",
            "104\n",
            "99\n",
            "99\n",
            "5.648593759433615\n",
            "11667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cur=datetime.datetime.now()\n",
        "predict(\n",
        "detection_model = detection_model,\n",
        "source='/content/gdrive/MyDrive/d3.mp4',\n",
        "slice_height=288,\n",
        "slice_width=512,\n",
        "overlap_height_ratio=0.2,\n",
        "overlap_width_ratio=0.2,\n",
        "return_dict=True,\n",
        "view_video=False,\n",
        "postprocess_type ='NMS'\n",
        ")\n",
        "now=datetime.datetime.now()\n",
        "print(122/(now-cur).total_seconds())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sydBATKPadd-",
        "outputId": "9a65a6cc-4ce4-4a89-f0c6-d018c9726ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:   0%|          | 0/122 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:   1%|          | 1/122 [00:01<03:22,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1604.03 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:   2%|▏         | 2/122 [00:02<02:49,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1169.42 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:   2%|▏         | 3/122 [00:04<02:40,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1219.37 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:   3%|▎         | 4/122 [00:05<02:35,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1213.95 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:   4%|▍         | 5/122 [00:06<02:32,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1214.29 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:   5%|▍         | 6/122 [00:08<02:30,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1217.21 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:   6%|▌         | 7/122 [00:09<02:27,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1181.43 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:   7%|▋         | 8/122 [00:10<02:26,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1213.78 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:   7%|▋         | 9/122 [00:11<02:25,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1215.48 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:   8%|▊         | 10/122 [00:13<02:23,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1191.58 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:   9%|▉         | 11/122 [00:14<02:22,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1207.64 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  10%|▉         | 12/122 [00:15<02:22,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1224.11 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  11%|█         | 13/122 [00:17<02:21,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1219.04 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  11%|█▏        | 14/122 [00:18<02:20,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1223.79 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  12%|█▏        | 15/122 [00:19<02:20,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1249.28 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  13%|█▎        | 16/122 [00:21<02:18,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1211.41 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  14%|█▍        | 17/122 [00:22<02:18,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1241.20 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  15%|█▍        | 18/122 [00:23<02:18,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1263.23 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  16%|█▌        | 19/122 [00:25<02:17,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1241.81 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  16%|█▋        | 20/122 [00:26<02:16,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1225.30 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  17%|█▋        | 21/122 [00:27<02:14,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1205.44 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  18%|█▊        | 22/122 [00:29<02:12,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1205.09 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  19%|█▉        | 23/122 [00:30<02:11,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1201.54 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  20%|█▉        | 24/122 [00:31<02:10,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1224.04 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  20%|██        | 25/122 [00:33<02:14,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1385.30 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  21%|██▏       | 26/122 [00:34<02:21,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1554.76 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  22%|██▏       | 27/122 [00:36<02:17,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1233.60 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  23%|██▎       | 28/122 [00:37<02:13,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1219.90 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  24%|██▍       | 29/122 [00:38<02:08,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1178.33 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  25%|██▍       | 30/122 [00:40<02:04,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1161.92 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  25%|██▌       | 31/122 [00:41<02:00,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1120.64 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  26%|██▌       | 32/122 [00:42<01:58,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1173.52 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  27%|██▋       | 33/122 [00:44<01:57,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1194.08 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  28%|██▊       | 34/122 [00:45<01:57,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1203.25 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  29%|██▊       | 35/122 [00:46<01:56,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1194.53 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  30%|██▉       | 36/122 [00:48<01:56,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1240.40 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  30%|███       | 37/122 [00:49<01:55,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1196.75 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  31%|███       | 38/122 [00:51<01:54,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1210.50 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  32%|███▏      | 39/122 [00:52<01:53,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1218.50 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  33%|███▎      | 40/122 [00:53<01:52,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1215.05 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  34%|███▎      | 41/122 [00:55<01:50,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1198.24 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  34%|███▍      | 42/122 [00:56<01:49,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1193.27 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  35%|███▌      | 43/122 [00:57<01:48,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1222.75 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  36%|███▌      | 44/122 [00:59<01:45,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1151.90 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  37%|███▋      | 45/122 [01:00<01:44,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1177.26 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  38%|███▊      | 46/122 [01:01<01:43,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1200.90 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  39%|███▊      | 47/122 [01:03<01:41,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1151.08 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  39%|███▉      | 48/122 [01:04<01:41,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1201.17 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1174.85 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  40%|████      | 49/122 [01:06<01:39,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  41%|████      | 50/122 [01:07<01:39,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1204.09 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  42%|████▏     | 51/122 [01:08<01:39,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1240.67 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  43%|████▎     | 52/122 [01:10<01:37,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1185.37 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  43%|████▎     | 53/122 [01:11<01:36,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1216.08 ms\n",
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  44%|████▍     | 54/122 [01:12<01:35,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1217.69 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  44%|████▍     | 54/122 [01:13<01:35,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  45%|████▌     | 55/122 [01:14<01:34,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1223.29 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  45%|████▌     | 55/122 [01:14<01:34,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  46%|████▌     | 56/122 [01:15<01:33,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1205.02 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  46%|████▌     | 56/122 [01:16<01:33,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  47%|████▋     | 57/122 [01:17<01:33,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1237.99 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  47%|████▋     | 57/122 [01:17<01:33,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  48%|████▊     | 58/122 [01:18<01:31,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1216.84 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  48%|████▊     | 58/122 [01:18<01:31,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  48%|████▊     | 59/122 [01:20<01:30,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1220.08 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  48%|████▊     | 59/122 [01:20<01:30,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  49%|████▉     | 60/122 [01:21<01:27,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1137.49 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  49%|████▉     | 60/122 [01:21<01:27,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  50%|█████     | 61/122 [01:22<01:26,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1231.55 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  50%|█████     | 61/122 [01:23<01:26,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  51%|█████     | 62/122 [01:24<01:26,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1238.64 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  51%|█████     | 62/122 [01:24<01:26,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  52%|█████▏    | 63/122 [01:25<01:25,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1227.10 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  52%|█████▏    | 63/122 [01:26<01:25,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  52%|█████▏    | 64/122 [01:27<01:24,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1235.20 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  52%|█████▏    | 64/122 [01:27<01:24,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  53%|█████▎    | 65/122 [01:28<01:22,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1210.93 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  53%|█████▎    | 65/122 [01:29<01:22,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  54%|█████▍    | 66/122 [01:30<01:21,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1234.50 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  54%|█████▍    | 66/122 [01:30<01:21,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  55%|█████▍    | 67/122 [01:31<01:20,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1221.90 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  55%|█████▍    | 67/122 [01:31<01:20,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  56%|█████▌    | 68/122 [01:33<01:18,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1199.88 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  56%|█████▌    | 68/122 [01:33<01:18,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  57%|█████▋    | 69/122 [01:34<01:17,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1231.00 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  57%|█████▋    | 69/122 [01:34<01:17,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  57%|█████▋    | 70/122 [01:36<01:16,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1222.72 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  57%|█████▋    | 70/122 [01:36<01:16,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  58%|█████▊    | 71/122 [01:37<01:15,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1227.44 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  58%|█████▊    | 71/122 [01:37<01:15,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  59%|█████▉    | 72/122 [01:39<01:14,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1205.33 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  59%|█████▉    | 72/122 [01:39<01:14,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  60%|█████▉    | 73/122 [01:40<01:12,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1213.97 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  60%|█████▉    | 73/122 [01:40<01:12,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  61%|██████    | 74/122 [01:42<01:11,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1234.25 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  61%|██████    | 74/122 [01:42<01:11,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  61%|██████▏   | 75/122 [01:43<01:10,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1252.64 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  61%|██████▏   | 75/122 [01:43<01:10,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  62%|██████▏   | 76/122 [01:45<01:08,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1218.46 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  62%|██████▏   | 76/122 [01:45<01:08,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  63%|██████▎   | 77/122 [01:46<01:07,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1208.00 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  63%|██████▎   | 77/122 [01:46<01:07,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  64%|██████▍   | 78/122 [01:48<01:06,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1247.75 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  64%|██████▍   | 78/122 [01:48<01:06,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  65%|██████▍   | 79/122 [01:49<01:04,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1239.29 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  65%|██████▍   | 79/122 [01:49<01:04,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  66%|██████▌   | 80/122 [01:51<01:03,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1205.26 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  66%|██████▌   | 80/122 [01:51<01:03,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  66%|██████▋   | 81/122 [01:52<01:01,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1233.25 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  66%|██████▋   | 81/122 [01:52<01:01,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  67%|██████▋   | 82/122 [01:54<01:00,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1187.50 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  67%|██████▋   | 82/122 [01:54<01:00,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  68%|██████▊   | 83/122 [01:55<00:58,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1217.34 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  68%|██████▊   | 83/122 [01:55<00:58,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  69%|██████▉   | 84/122 [01:57<00:57,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1193.23 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  69%|██████▉   | 84/122 [01:57<00:57,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  70%|██████▉   | 85/122 [01:58<00:55,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1213.13 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  70%|██████▉   | 85/122 [01:58<00:55,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  70%|███████   | 86/122 [02:00<00:55,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1241.92 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  70%|███████   | 86/122 [02:00<00:55,  1.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  71%|███████▏  | 87/122 [02:01<00:53,  1.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1217.52 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  71%|███████▏  | 87/122 [02:02<00:53,  1.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  72%|███████▏  | 88/122 [02:03<00:52,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1255.97 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  72%|███████▏  | 88/122 [02:03<00:52,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  73%|███████▎  | 89/122 [02:04<00:50,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1230.10 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  73%|███████▎  | 89/122 [02:05<00:50,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  74%|███████▍  | 90/122 [02:06<00:49,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1239.29 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  74%|███████▍  | 90/122 [02:06<00:49,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  75%|███████▍  | 91/122 [02:08<00:48,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1254.04 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  75%|███████▍  | 91/122 [02:08<00:48,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  75%|███████▌  | 92/122 [02:09<00:46,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1247.94 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  75%|███████▌  | 92/122 [02:09<00:46,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  76%|███████▌  | 93/122 [02:11<00:44,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1198.17 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  76%|███████▌  | 93/122 [02:11<00:44,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  77%|███████▋  | 94/122 [02:12<00:43,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1224.44 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  77%|███████▋  | 94/122 [02:13<00:43,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  78%|███████▊  | 95/122 [02:14<00:42,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1245.88 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  78%|███████▊  | 95/122 [02:14<00:42,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  79%|███████▊  | 96/122 [02:15<00:41,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1354.65 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  79%|███████▊  | 96/122 [02:16<00:41,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  80%|███████▉  | 97/122 [02:17<00:39,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1225.74 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  80%|███████▉  | 97/122 [02:17<00:39,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  80%|████████  | 98/122 [02:19<00:37,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1214.63 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  80%|████████  | 98/122 [02:19<00:37,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  81%|████████  | 99/122 [02:20<00:36,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1261.68 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  81%|████████  | 99/122 [02:21<00:36,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  82%|████████▏ | 100/122 [02:22<00:34,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1225.88 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  82%|████████▏ | 100/122 [02:22<00:34,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  83%|████████▎ | 101/122 [02:23<00:33,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1263.14 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  83%|████████▎ | 101/122 [02:24<00:33,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  84%|████████▎ | 102/122 [02:25<00:31,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1188.86 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  84%|████████▎ | 102/122 [02:25<00:31,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  84%|████████▍ | 103/122 [02:27<00:30,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1235.74 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  84%|████████▍ | 103/122 [02:27<00:30,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  85%|████████▌ | 104/122 [02:28<00:28,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1255.47 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  85%|████████▌ | 104/122 [02:28<00:28,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  86%|████████▌ | 105/122 [02:30<00:27,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1260.52 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  86%|████████▌ | 105/122 [02:30<00:27,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  87%|████████▋ | 106/122 [02:31<00:25,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1242.23 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  87%|████████▋ | 106/122 [02:32<00:25,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  88%|████████▊ | 107/122 [02:33<00:24,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1250.23 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  88%|████████▊ | 107/122 [02:33<00:24,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  89%|████████▊ | 108/122 [02:35<00:22,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1238.64 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  89%|████████▊ | 108/122 [02:35<00:22,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  89%|████████▉ | 109/122 [02:36<00:20,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1254.58 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  89%|████████▉ | 109/122 [02:37<00:20,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  90%|█████████ | 110/122 [02:38<00:19,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1262.85 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  90%|█████████ | 110/122 [02:38<00:19,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  91%|█████████ | 111/122 [02:40<00:17,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1258.20 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  91%|█████████ | 111/122 [02:40<00:17,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  92%|█████████▏| 112/122 [02:41<00:16,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1248.82 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  92%|█████████▏| 112/122 [02:42<00:16,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  93%|█████████▎| 113/122 [02:43<00:14,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1256.73 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  93%|█████████▎| 113/122 [02:43<00:14,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  93%|█████████▎| 114/122 [02:44<00:13,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1244.34 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  93%|█████████▎| 114/122 [02:45<00:13,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  94%|█████████▍| 115/122 [02:46<00:11,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1261.47 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  94%|█████████▍| 115/122 [02:46<00:11,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  95%|█████████▌| 116/122 [02:48<00:09,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1244.43 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  95%|█████████▌| 116/122 [02:48<00:09,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  96%|█████████▌| 117/122 [02:49<00:08,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1259.41 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  96%|█████████▌| 117/122 [02:50<00:08,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  97%|█████████▋| 118/122 [02:51<00:06,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1251.71 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  97%|█████████▋| 118/122 [02:51<00:06,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  98%|█████████▊| 119/122 [02:53<00:04,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1249.80 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  98%|█████████▊| 119/122 [02:53<00:04,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  98%|█████████▊| 120/122 [02:54<00:03,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1242.30 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  98%|█████████▊| 120/122 [02:55<00:03,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  99%|█████████▉| 121/122 [02:56<00:01,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1237.48 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames:  99%|█████████▉| 121/122 [02:56<00:01,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing prediction on 36 number of slices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing inference on video frames: 100%|██████████| 122/122 [02:58<00:00,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction time is: 1263.69 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPerforming inference on video frames: 100%|██████████| 122/122 [02:58<00:00,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=========================== Video Ended ===========================\n",
            "Prediction results are successfully exported to runs/predict/exp\n",
            "0.6829889646058044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NA-34iktxiYh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}